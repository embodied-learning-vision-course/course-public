{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e75d37-c08b-4054-9b9c-77fc6832a519",
   "metadata": {},
   "source": [
    "# Advanced Topics in Embodied Learning and Vision: Video Learning Demo\n",
    "##### 2025-02-06, Chris Hoang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867bed1-c69d-4c1a-b7ac-6bb565a26cf6",
   "metadata": {},
   "source": [
    "### Setup and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745bcf70-bb15-4614-b3b6-0c64dfe4a9d8",
   "metadata": {},
   "source": [
    "1. Create a new conda environment (remember to modify your Jupyter setup to activate this conda environment)\n",
    "```\n",
    "conda create -n \"video\" python=3.10.0\n",
    "```\n",
    "2. Install torch, torchvision, decord, ipykernel\n",
    "```\n",
    "pip install torch==2.2.0 torchvision==0.17 --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install decord\n",
    "pip install ipykernel\n",
    "```\n",
    "3. Copy BDD100K example videos\n",
    "\n",
    "```\n",
    "scp /scratch/ch3451/evl/video-learning-demo/videos <YOUR_DIR>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d97c1b8-830e-4886-a3f7-1b05b8e4d6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as tvF\n",
    "from decord import VideoReader, cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52279d1-5fc2-4f63-a2ca-33adab666581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDD100KDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 delta_t,\n",
    "                 repeat_sample=None,\n",
    "                 crop_scale=None,\n",
    "                 decode_resolution=None,\n",
    "                 ):\n",
    "        self.root_dir = root_dir\n",
    "        self.repeat_sample = repeat_sample or 1\n",
    "        self.delta_t = delta_t\n",
    "        self.decode_resolution = decode_resolution\n",
    "\n",
    "        self.video_paths = sorted([os.path.join(root_dir, f) for f in os.listdir(root_dir)])\n",
    "        self._dataset_len = len(self.video_paths)\n",
    "\n",
    "        # Define data augmentations\n",
    "        def transform(x, y):\n",
    "            if crop_scale is not None:\n",
    "                random_crop = transforms.Compose([\n",
    "                    transforms.RandomResizedCrop((224, 224), scale=(0.2, 0.4), interpolation=Image.BILINEAR),\n",
    "                    transforms.ToTensor()\n",
    "                ])\n",
    "                x1 = random_crop(x)\n",
    "                x2 = random_crop(y)\n",
    "                y1 = random_crop(y)\n",
    "                y2 = random_crop(y)\n",
    "                return [x1, x2, y1, y2]\n",
    "            else:\n",
    "                return [tvF.to_tensor(x), tvF.to_tensor(y)]\n",
    "        self.transform = transform\n",
    "       \n",
    "    def __len__(self):\n",
    "        return self._dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve CPU worker to use for Decord VideoReader\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        cpuid = 0 if worker_info == None else int(worker_info.id)\n",
    "\n",
    "        # Potentially decode into lower resolution\n",
    "        if self.decode_resolution is not None:\n",
    "            h, w = self.decode_resolution\n",
    "            vr = VideoReader(self.video_paths[idx], num_threads=0, ctx=cpu(cpuid), width=w, height=h)\n",
    "        else:\n",
    "            vr = VideoReader(self.video_paths[idx], num_threads=0, ctx=cpu(cpuid))\n",
    "        vr_len = len(vr)\n",
    "\n",
    "        # Get random frame indices as well as future frame indices to decode\n",
    "        i_s = np.random.randint(0, vr_len - self.delta_t[1], size=self.repeat_sample)\n",
    "        delta_ts = np.random.randint(self.delta_t[0], self.delta_t[1]+1, size=self.repeat_sample)\n",
    "        i_s = np.array([index for i, delta_t in zip(i_s, delta_ts) for index in [i, i+delta_t]])\n",
    "\n",
    "        # Sort frame indices to decode frame in-order, which is faster\n",
    "        sort_indexes = np.argsort(i_s).astype(np.int32)\n",
    "        unsort_indexes = np.argsort(sort_indexes).astype(np.int32)\n",
    "\n",
    "        try:\n",
    "            imgs = vr.get_batch(list(i_s[sort_indexes])).asnumpy()[unsort_indexes]\n",
    "            del vr; gc.collect()\n",
    "\n",
    "            ls = []\n",
    "            # Augment pairs of frames at a time\n",
    "            for i in range(self.repeat_sample):\n",
    "                i1, i2 = i*2, i*2+1\n",
    "                img1 = tvF.to_pil_image(imgs[i1])\n",
    "                img2 = tvF.to_pil_image(imgs[i2])\n",
    "                aug_img = self.transform(img1, img2)\n",
    "                if len(ls) == 0:\n",
    "                    ls = [[] for _ in range(len(aug_img))]\n",
    "                for j in range(len(aug_img)):\n",
    "                    ls[j].append(aug_img[j])\n",
    "\n",
    "            return [torch.stack(l, dim=0) for l in ls]\n",
    "        except Exception as e:\n",
    "            # If failure, decode from a different randomly sampled video\n",
    "            print(f\"Error reading video {self.video_paths[idx]}: {e}\")\n",
    "            return self.__getitem__(np.random.randint(0, self._dataset_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a1d43-1eb7-4145-b6d1-a14fb81aa6b5",
   "metadata": {},
   "source": [
    "#### Sample data from BDD videos, high resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c3c65f-60aa-44ab-ab30-1d81c6fb8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BDD100KDataset(\n",
    "    '/scratch/ch3451/evl/video-learning-demo/videos',\n",
    "    delta_t=[5,5],\n",
    "    repeat_sample=4,\n",
    "    crop_scale=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b72b547-5a0d-456e-882c-c87289c138bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[-1]\n",
    "example = torch.cat(example, dim=0)\n",
    "grid = torchvision.utils.make_grid((example * 255.).type(torch.uint8), nrow=2, padding=2, normalize=False, pad_value=0)\n",
    "torchvision.io.write_png(grid, 'high-resolution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b7252-0908-4b28-85fe-c11fa6fe6e45",
   "metadata": {},
   "source": [
    "#### Using no CPU workers for dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "076021fa-33eb-44aa-8253-e3b91bd62671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.RandomSampler(dataset)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    sampler=sampler,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425918ee-ba3c-4259-8ec4-f3308137ae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 12.720113277435303\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for batch in data_loader:\n",
    "    continue\n",
    "end_time = time.time()\n",
    "print(f'Total time: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83053ae1-d814-41ef-b5d7-3bc1cda68b88",
   "metadata": {},
   "source": [
    "#### Using 12 CPU workers to speed up dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dfe7765-98d1-4725-aa21-7d24cb6d6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    sampler=sampler,\n",
    "    batch_size=1,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49813896-d2b9-4dcd-8b98-d7867261574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 2.587550401687622\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for batch in data_loader:\n",
    "    continue\n",
    "end_time = time.time()\n",
    "print(f'Total time: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638b633-3b5e-4cca-b14b-aa1843705961",
   "metadata": {},
   "source": [
    "#### Decoding to a lower resolution to speed up dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5be8db-806e-4657-be6e-0b6890704839",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BDD100KDataset(\n",
    "    '/scratch/ch3451/evl/video-learning-demo/videos',\n",
    "    delta_t=[5,5],\n",
    "    repeat_sample=4,\n",
    "    decode_resolution=(180,320),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec114f6-cafd-4350-a85e-bf05156597e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.RandomSampler(dataset)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    sampler=sampler,\n",
    "    batch_size=1,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e4f4568-e509-472c-a102-1eb3d2e00c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.5015265941619873\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for batch in data_loader:\n",
    "    continue\n",
    "end_time = time.time()\n",
    "print(f'Total time: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8b8a72-7883-4229-b4ff-d683989bf5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[-1]\n",
    "example = torch.cat(example, dim=0)\n",
    "grid = torchvision.utils.make_grid((example * 255.).type(torch.uint8), nrow=4, padding=2, normalize=False, pad_value=0)\n",
    "torchvision.io.write_png(grid, 'low-resolution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35570a4d-e597-4b06-af58-2167368cdd45",
   "metadata": {},
   "source": [
    "#### Using RandomResizedCrop data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9a5294-6b5f-4b5f-a5ca-831418ec0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BDD100KDataset(\n",
    "    '/scratch/ch3451/evl/video-learning-demo/videos',\n",
    "    delta_t=[5,5],\n",
    "    repeat_sample=4,\n",
    "    crop_scale=(0.2, 0.4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31cf4375-2bd0-43fc-a7a7-d5e90e698bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[-1]\n",
    "example = torch.cat(example, dim=0)\n",
    "grid = torchvision.utils.make_grid((example * 255.).type(torch.uint8), nrow=4, padding=2, normalize=False, pad_value=0)\n",
    "torchvision.io.write_png(grid, 'random-crop.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bcabb9-f328-4303-b331-b2ffeb8e340a",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "decord: https://github.com/dmlc/decord/blob/master/examples/video_reader.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
