{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f4f6dc-502e-4a66-98b3-225b39ae0df6",
   "metadata": {},
   "source": [
    "# Advanced Topics in Embodied Learning and Vision: Habitat Navigation Demo\n",
    "##### 2025-01-30, Chris Hoang\n",
    "\n",
    "Tutorial materials derived from https://aihabitat.org/tutorial/2020 and https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b7e6a-5d88-40ee-90e2-3408ce00cfba",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d0228-09e9-436a-b4c7-62a941f7823f",
   "metadata": {},
   "source": [
    "1. Follow https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene/software/singularity-with-miniconda to create a Singularity container and conda environment\n",
    "\n",
    "2. Create Habitat conda environment\n",
    "```\n",
    "conda create -n habitat python=3.9 cmake=3.14.0\n",
    "conda activate habitat\n",
    "```\n",
    "\n",
    "3. Install pytorch\n",
    "```\n",
    "pip install torch==2.0.0 torchvision==0.15.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "4. Install habitat-sim\n",
    "```\n",
    "conda install habitat-sim withbullet -c conda-forge -c aihabitat\n",
    "```\n",
    "\n",
    "5. Install habitat-lab and habitat-baselines\n",
    "```\n",
    "git clone --branch stable https://github.com/facebookresearch/habitat-lab.git\n",
    "cd habitat-lab\n",
    "pip install -e habitat-lab  # install habitat_lab\n",
    "pip install -e habitat-baselines  # install habitat_baselines\n",
    "```\n",
    "\n",
    "6. Download Habitat test scenes\n",
    "```\n",
    "!wget -q https://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip\n",
    "!unzip -q habitat-test-scenes.zip\n",
    "```\n",
    "\n",
    "7. Transfer Habitat demo files to your scratch\n",
    "```\n",
    "export BASE_DIR=<dir containing habitat-lab>\n",
    "cp /scratch/ch3451/evl/habitat-demo/habitat-demo.ipynb $BASE_DIR\n",
    "cp /scratch/ch3451/evl/habitat-demo/habitat-demo.yaml $BASE_DIR\n",
    "cp /scratch/ch3451/evl/habitat-demo/habitat-demo.json.gz $BASE_DIR/data/datasets/pointnav/habitat-test-scenes/v1/train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf0f4a-99e5-47f9-be31-c08cdd04a86e",
   "metadata": {},
   "source": [
    "## Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca3c9d-9ee9-4a7d-a017-1e73bb1c6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR='/scratch/ch3451/evl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca4260-d5af-41ef-b9ed-2d40227c8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d0166-2dac-4004-b214-1be4a65acac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import git\n",
    "from gym import spaces\n",
    "import magnum as mn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as tvF\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%cd $BASE_DIR/\"habitat-lab\"\n",
    "# repo = git.Repo(\".\", search_parent_directories=True)\n",
    "# dir_path = repo.working_tree_dir\n",
    "# %cd $dir_path\n",
    "\n",
    "import habitat\n",
    "from habitat import Env, get_config\n",
    "from habitat.config.default_structured_configs import ActionConfig\n",
    "from habitat.core.logging import logger\n",
    "from habitat.core.registry import registry\n",
    "from habitat.sims.habitat_simulator.actions import HabitatSimActions\n",
    "from habitat.tasks.nav.nav import NavigationTask, SimulatorTaskAction\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "from habitat_baselines.config.default import get_config as get_baselines_config\n",
    "from habitat.config.default_structured_configs import ActionConfig\n",
    "\n",
    "def display_sample(\n",
    "    rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([]), rgb_name='rgb'\n",
    "):  # noqa B006\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGB\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [rgb_name]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\n",
    "            \"P\", (semantic_obs.shape[1], semantic_obs.shape[0])\n",
    "        )\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray(\n",
    "            (depth_obs / 10 * 255).astype(np.uint8), mode=\"L\"\n",
    "        )\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee339dd-52c6-4fd4-89e2-e207f23634c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Walkthrough environment using simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d8254-9ffc-402e-aaa1-3e310329477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $BASE_DIR\n",
    "!cat './habitat-demo.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5505e-4f0f-4278-bf0f-9822c2bb195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = habitat.get_config(\n",
    "    config_path=\"./habitat-demo.yaml\"\n",
    ")\n",
    "\n",
    "env = habitat.Env(config=config)\n",
    "action = None\n",
    "obs = env.reset()\n",
    "valid_actions = [\"turn_left\", \"turn_right\", \"move_forward\"]\n",
    "num_steps = 15\n",
    "for i in range(num_steps):\n",
    "    display_sample(obs[\"rgb\"])\n",
    "    display_sample(obs[\"imagegoal\"], rgb_name='imagegoal')\n",
    "    metrics = env.get_metrics()\n",
    "    print(\n",
    "        \"distance to goal: {:.2f}\".format(\n",
    "            metrics[\"distance_to_goal\"]\n",
    "        )\n",
    "    )\n",
    "    action = random.sample(valid_actions, 1)[0]\n",
    "    print(action)\n",
    "    obs = env.step(\n",
    "        {\n",
    "            \"action\": action,\n",
    "        }\n",
    "    )\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdaeb24-508f-4a93-857b-5c315cd0bbb2",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ae8b4-4774-4b1a-8976-f3c006b2c2b6",
   "metadata": {},
   "source": [
    "#### Helper classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639ee5a-129a-41a1-b329-ded5ce3fe05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def extract_obs(x, device=torch.device('cpu')):\n",
    "    x = torch.Tensor(x).float()\n",
    "    x = x[None, :].permute(0, 3, 1, 2)\n",
    "    x = x / 255.\n",
    "    x = tvF.normalize(x, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba616c-c95a-4199-ab20-1ae5b79b6ba6",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f591735-f51f-4221-9fb8-ea9d1d4bb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        z_img = self.encoder(img)\n",
    "        q_values = self.mlp(z_img.mean(dim=(2, 3)))\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa043f-e842-4de2-a71c-0e841ffc5f41",
   "metadata": {},
   "source": [
    "#### Custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a34045-19d9-4fda-9a75-4be06c5a86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StrafeActionConfig(ActionConfig):\n",
    "    move_amount: float = 0.0  # We will change this in the configuration\n",
    "    noise_amount: float = 0.0\n",
    "    angle: float = 0.0\n",
    "\n",
    "\n",
    "# This is a helper that implements strafing that we will use in our actions\n",
    "def _strafe_body(\n",
    "    sim,\n",
    "    move_amount: float,\n",
    "    strafe_angle_deg: float,\n",
    "    noise_amount: float,\n",
    "):\n",
    "    # Get the state of the agent\n",
    "    agent_state = sim.get_agent_state()\n",
    "\n",
    "    # Convert from np.quaternion to mn.Quaternion\n",
    "    normalized_quaternion = agent_state.rotation\n",
    "    agent_mn_quat = mn.Quaternion(\n",
    "        normalized_quaternion.imag, normalized_quaternion.real\n",
    "    )\n",
    "\n",
    "    # Apply noise to strafe angle\n",
    "    strafe_angle = np.random.uniform(\n",
    "        (1 - noise_amount) * strafe_angle_deg,\n",
    "        (1 + noise_amount) * strafe_angle_deg,\n",
    "    )\n",
    "    strafe_angle = mn.Deg(strafe_angle)\n",
    "\n",
    "    # Calculate relative rotation\n",
    "    rotation = agent_mn_quat * \\\n",
    "        mn.Quaternion.rotation(strafe_angle, mn.Vector3.y_axis())\n",
    "\n",
    "    # Apply noise to move amount\n",
    "    move_amount = np.random.uniform(\n",
    "        (1 - noise_amount) * move_amount, (1 + noise_amount) * move_amount\n",
    "    )\n",
    "\n",
    "    # Calculate new position\n",
    "    forward = rotation.transform_vector(-mn.Vector3.z_axis())\n",
    "    delta_position = forward * move_amount\n",
    "    final_position = sim.pathfinder.try_step(  # type: ignore\n",
    "        agent_state.position, agent_state.position + delta_position\n",
    "    )\n",
    "\n",
    "    # Set the new state of the agent\n",
    "    sim.set_agent_state(\n",
    "        final_position,\n",
    "        [*rotation.vector, rotation.scalar],\n",
    "        reset_sensors=False,\n",
    "    )\n",
    "\n",
    "# We define and register our actions as follows.\n",
    "# the __init__ method receives a sim and config argument.\n",
    "@habitat.registry.register_task_action\n",
    "class MoveForward(SimulatorTaskAction):\n",
    "    def __init__(self, *args, config, sim, **kwargs):\n",
    "        super().__init__(*args, config=config, sim=sim, **kwargs)\n",
    "        self._sim = sim\n",
    "        self._move_amount = config.move_amount\n",
    "        self._noise_amount = config.noise_amount\n",
    "\n",
    "    def _get_uuid(self, *args, **kwargs) -> str:\n",
    "        return \"move_forward\"\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        # print(\n",
    "        #     f\"Calling {self._get_uuid()} d={self._move_amount}m noise={self._noise_amount}\"\n",
    "        # )\n",
    "        # This is where the code for the new action goes. Here we use a\n",
    "        # helper method but you could directly modify the simulation here.\n",
    "        _strafe_body(self._sim, self._move_amount, 0, self._noise_amount)\n",
    "\n",
    "@habitat.registry.register_task_action\n",
    "class MoveLeft(SimulatorTaskAction):\n",
    "    def __init__(self, *args, config, sim, **kwargs):\n",
    "        super().__init__(*args, config=config, sim=sim, **kwargs)\n",
    "        self._sim = sim\n",
    "        self._move_amount = config.move_amount\n",
    "        self._angle = config.angle\n",
    "        self._noise_amount = config.noise_amount\n",
    "\n",
    "    def _get_uuid(self, *args, **kwargs) -> str:\n",
    "        return \"move_left\"\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        # print(\n",
    "        #     f\"Calling {self._get_uuid()} d={self._move_amount}m noise={self._noise_amount} angle={self._angle}\"\n",
    "        # )\n",
    "        # This is where the code for the new action goes. Here we use a\n",
    "        # helper method but you could directly modify the simulation here.\n",
    "        _strafe_body(self._sim, self._move_amount, self._angle, self._noise_amount)\n",
    "\n",
    "\n",
    "@habitat.registry.register_task_action\n",
    "class MoveRight(SimulatorTaskAction):\n",
    "    def __init__(self, *args, config, sim, **kwargs):\n",
    "        super().__init__(*args, config=config, sim=sim, **kwargs)\n",
    "        self._sim = sim\n",
    "        self._move_amount = config.move_amount\n",
    "        self._angle = config.angle\n",
    "        self._noise_amount = config.noise_amount\n",
    "\n",
    "    def _get_uuid(self, *args, **kwargs) -> str:\n",
    "        return \"move_right\"\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        # print(\n",
    "        #     f\"Calling {self._get_uuid()} d={self._move_amount}m noise={self._noise_amount} angle={-self._angle}\"\n",
    "        # )\n",
    "        _strafe_body(self._sim, self._move_amount, -self._angle, self._noise_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6abe2d-3f3a-4375-a2e4-89a8278e4f65",
   "metadata": {},
   "source": [
    "#### Customize environment actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9858ae6-6308-4e55-98d1-90844dcd9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $BASE_DIR\n",
    "config = habitat.get_config(\n",
    "    config_path=\"./habitat-demo.yaml\"\n",
    ")\n",
    "\n",
    "with habitat.read_write(config):\n",
    "    config.habitat.task.actions = {}\n",
    "    config.habitat.task.actions[\"MOVE_FORWARD\"] = StrafeActionConfig(\n",
    "            type=\"MoveForward\",\n",
    "            move_amount=0.25,\n",
    "            noise_amount=0.0,\n",
    "        )\n",
    "    config.habitat.task.actions[\"MOVE_LEFT\"] = StrafeActionConfig(\n",
    "            type=\"MoveLeft\",\n",
    "            move_amount=0.25,\n",
    "            noise_amount=0.0,\n",
    "            angle=90\n",
    "        )\n",
    "    config.habitat.task.actions[\"MOVE_RIGHT\"] = StrafeActionConfig(\n",
    "            type=\"MoveRight\",\n",
    "            move_amount=0.25,\n",
    "            noise_amount=0.0,\n",
    "            angle=90\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4442205-4225-4c29-aa75-53e29fa22b50",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81831c63-6621-457b-94f0-ef243efe265c",
   "metadata": {},
   "source": [
    "#### Setup environment and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca3d36-0358-4036-a427-6a066c2d81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-4\n",
    "batch_size = 16\n",
    "gamma = 0.99\n",
    "eps_start = 0.50\n",
    "eps_end = 0.01\n",
    "eps_decay = 100\n",
    "tau = 0.005\n",
    "num_episodes = 30\n",
    "max_steps = 50\n",
    "\n",
    "env = habitat.gym.make_gym_from_config(config=config)\n",
    "env.seed(42)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "device = torch.device('cuda')\n",
    "model = DQN(num_actions).to(device)\n",
    "target_model = DQN(num_actions).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(capacity=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdc9de-3dd2-48cc-a3e9-c9c6c8114a20",
   "metadata": {},
   "source": [
    "#### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c5ca9-6e27-424a-aa26-e608736917d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Get the goal image\n",
    "obs = env.reset()\n",
    "goal_image = extract_obs(obs['imagegoal'], device)\n",
    "\n",
    "# Training loop\n",
    "steps_done = 0\n",
    "update_interval = 1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    img = extract_obs(state['rgb'])\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_loss = 0\n",
    "    min_dist_to_goal = 1000\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Select action\n",
    "        eps_threshold = eps_end + (eps_start - eps_end) * np.exp(-1. * steps_done / eps_decay)\n",
    "        steps_done += 1\n",
    "        if np.random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                q_values = model(img.to(device))\n",
    "                if t == 0:\n",
    "                    print(f'MoveForward q_value: {q_values[0][0]:.4f}')\n",
    "                    print(f'MoveLeft q_value: {q_values[0][1]:.4f}')\n",
    "                    print(f'MoveRight q_value: {q_values[0][2]:.3f}')\n",
    "                action = torch.argmax(q_values, dim=1).to('cpu')\n",
    "        else:\n",
    "            action = np.random.choice(num_actions, size=1)\n",
    "            action = torch.tensor(action) # Convert back to tensor to be consistent with other action types\n",
    "\n",
    "        # Execute action\n",
    "        next_state, reward, done, info = env.step(action.item())\n",
    "        next_img = extract_obs(next_state['rgb'])\n",
    "\n",
    "        if info['distance_to_goal'] < 0.3:\n",
    "            reward = 100\n",
    "            print('success!')\n",
    "            done = True\n",
    "        min_dist_to_goal = min(min_dist_to_goal, info['distance_to_goal'])\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        replay_buffer.add(img, action, reward, next_img, done)\n",
    "\n",
    "        # Sample a batch from the replay buffer\n",
    "        if len(replay_buffer) > batch_size and (t % update_interval) == 0:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Convert the batch to tensors\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions = torch.cat(actions).to(device)\n",
    "            rewards = torch.tensor(rewards, device=device)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.bool, device=device)\n",
    "\n",
    "            # Compute the target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_state_values = target_model(next_states).max(1)[0]\n",
    "                target_q_values = rewards + (gamma * next_state_values * ~dones)\n",
    "\n",
    "            # Compute the expected Q-values\n",
    "            q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Compute the loss\n",
    "            criterion = nn.SmoothL1Loss()\n",
    "            loss = criterion(q_values, target_q_values)\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log loss for the current step\n",
    "            episode_loss += loss.item()\n",
    "            # if t % log_interval == 0:\n",
    "            #     print(f\"Episode: {episode}, Step: {t}, Loss: {loss.item()}\")\n",
    "\n",
    "            target_model_state_dict = target_model.state_dict()\n",
    "            model_state_dict = model.state_dict()\n",
    "            for key in model_state_dict:\n",
    "                target_model_state_dict[key] = model_state_dict[key]*tau + target_model_state_dict[key] * (1-tau)\n",
    "            target_model.load_state_dict(target_model_state_dict)\n",
    "\n",
    "        # Update episode reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        img = next_img\n",
    "        if done:\n",
    "            print('done!')\n",
    "            break\n",
    "\n",
    "    # Calculate and print average reward and loss for the episode\n",
    "    average_reward = episode_reward / (t+1)\n",
    "    average_loss = episode_loss / ((t+1) / update_interval)\n",
    "    print(f\"Episode: {episode}, Length: {t+1}, Reward: {average_reward:.3f}, Loss: {average_loss}, Eps: {eps_threshold:.3f}, Min dist to goal: {min_dist_to_goal:.3f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a21e-3003-4f03-b275-8fb79f7f34ed",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "From https://aihabitat.org/docs/habitat-lab/habitat-lab-tdmap-viz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185da0f-e0d3-4b19-89ea-8c62dd76d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habitat.utils.visualizations.utils import (\n",
    "    images_to_video,\n",
    "    observations_to_image,\n",
    "    overlay_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95052541-8503-4089-b973-b0e76fa97727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "%cd $BASE_DIR\n",
    "config = habitat.get_config(\n",
    "    config_path=\"./habitat-demo.yaml\"\n",
    ")\n",
    "with habitat.read_write(config):\n",
    "    config.habitat.task.actions = {}\n",
    "    config.habitat.task.actions[\"MOVE_FORWARD\"] = StrafeActionConfig(\n",
    "            type=\"MoveForward\",\n",
    "            move_amount=0.25,\n",
    "            noise_amount=0.0,\n",
    "        )\n",
    "    config.habitat.task.actions[\"MOVE_LEFT\"] = StrafeActionConfig(\n",
    "            type=\"MoveLeft\",\n",
    "            move_amount=0.25,\n",
    "            noise_amount=0.0,\n",
    "            angle=90\n",
    "        )\n",
    "    config.habitat.task.actions[\"MOVE_RIGHT\"] = StrafeActionConfig(\n",
    "            type=\"MoveRight\",\n",
    "            move_amount=0.25,\n",
    "            noise_amount=0.0,\n",
    "            angle=90\n",
    "        )\n",
    "\n",
    "# mode = 'random'\n",
    "mode = 'dqn'\n",
    "with habitat.Env(config=config) as env:\n",
    "    # Create video of agent navigating in the first episode\n",
    "    num_episodes = 1\n",
    "    for _ in range(num_episodes):\n",
    "        # Load the first episode\n",
    "        observations = env.reset()\n",
    "\n",
    "        # Get metrics\n",
    "        info = env.get_metrics()\n",
    "        # Concatenate RGB-D observation and topdowm map into one image\n",
    "        frame = observations_to_image(observations, info)\n",
    "\n",
    "        # Remove top_down_map from metrics\n",
    "        info.pop(\"top_down_map\")\n",
    "        # Overlay numeric metrics onto frame\n",
    "        frame = overlay_frame(frame, info)\n",
    "        # Add fame to vis_frames\n",
    "        vis_frames = [frame]\n",
    "\n",
    "        # Repeat the steps above while agent doesn't reach the goal\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done and not env.episode_over and i < 50:\n",
    "            i += 1\n",
    "            # Get the next best action\n",
    "            if mode == 'random':\n",
    "                action = random.randint(0, env.action_space.n - 1)\n",
    "                print(action)\n",
    "            elif mode == 'dqn':\n",
    "                action = model(extract_obs(obs['rgb'], device)).argmax(dim=1).item()\n",
    "            if action is None:\n",
    "                break\n",
    "\n",
    "            # Step in the environment\n",
    "            observations = env.step(action)\n",
    "            info = env.get_metrics()\n",
    "            if info['distance_to_goal'] < 0.3:\n",
    "                done = True\n",
    "            frame = observations_to_image(observations, info)\n",
    "\n",
    "            info.pop(\"top_down_map\")\n",
    "            frame = overlay_frame(frame, info)\n",
    "            vis_frames.append(frame)\n",
    "\n",
    "        print(f'Episode length {i}')\n",
    "        current_episode = env.current_episode\n",
    "        output_path = os.getcwd()\n",
    "        video_name = f'vis_{mode}'\n",
    "        # Create video from images and save to disk\n",
    "        images_to_video(\n",
    "            vis_frames, output_path, video_name, fps=1, quality=9\n",
    "        )\n",
    "        vis_frames.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdbbda0-665b-4e9d-97ba-054edd36d270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
